{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I2MAGZ2L7NJj"
      },
      "source": [
        "# ST446 Distributed Computing for Big Data\n",
        "## Assignment 2 - PART 2\n",
        "---\n",
        "\n",
        "## P2: Topic Modelling\n",
        "\n",
        "In this homework problem, you are asked to perform a semantic analysis of the DBLP author publications dataset `dblp/author_large.txt`.\n",
        "\n",
        "Please, refer to:\n",
        "* Week02 on how to download this dataset into the master node of your Dataproc cluster. You can put this dataset into a bucket on your GCP account and access it from your code as well.\n",
        "* Week09 on how to configure your Dataproc cluster to use the `NLTK` library.\n",
        "* Week09 on an example code for running LDA topic modelling.\n",
        "\n",
        "## Questions\n",
        "\n",
        "**P2.A (25 points)** Use Latent Dirichlet Allocation (LDA) to cluster publications by using words in their titles and represent each publication by 10 topics. Please follow these steps:\n",
        "\n",
        "**A.1** Convert titles to tokens by:\n",
        "   * Tokenizing words in the title of each publication.\n",
        "   * Removing stop words using the `nltk` package.\n",
        "   * Removing puctuations, numbers or other symbols.\n",
        "   * Lemmatizing tokens.\n",
        "\n",
        "Note that you may skip some of these editing steps or add some additional steps to edit the tokens, but if you do this provide a justification for it.\n",
        "\n",
        "**A.2** Convert tokens into sparse vectors.\n",
        "\n",
        "**A.3** Use LDA to find out 10 topics for each publication and represent each topic with the first few most relevant words. Note that you can choose to use different number of topics rather than 10. Again if you do so, please provide a justification.\n",
        "\n",
        "**A.4** Comment the obtained results.\n",
        "\n",
        "**P2.B (25 points)** Address each question as in part A, but with each *document* representing all publication tiles of a specific author. For example, if an author $Y$ wrote \"introduction to databases\" and \"database design\", then the *document* for the author $Y$ will be \"introduction to database database design\". \n",
        "\n",
        "In addition, calculate the **topic density** vector for each author and use the topic density to calculate the **cosine similarity** for each pair of authors. For example, if the topic density for author X is $[x_1, x_2, x_3, \\dots]$ and topic density vector for author Y is $[y_1, y_2, y_3, \\dots]$, then the cosine similarity is $\\frac{x_1\\cdot y_1 + x_2\\cdot y_2 + x_3\\cdot y_3 +\\dots}{\\sqrt{x_1^2+ x_2^2+ x_3^2 +\\dots}\\sqrt{y_1^2+ y_2^2+ y_3^2 +\\dots}}$. Show the 10 most similar author pairs and comment on their similarity, if possible taking into consideration the results from the previous section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJa1YyUG7NJl"
      },
      "source": [
        "## 0. Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KfcsGHg_7NJm",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# your code to adjust the path to your dataset author-large.txt\n",
        "author_rdd = sc.textFile('<PATH_TO>/author-large.txt', 4) \\\n",
        "                .map(lambda row: np.array(row.strip().split(\"\\t\")))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "ZtDwN5BV7NJm"
      },
      "outputs": [],
      "source": [
        "# example on how you can manipulate the RDD containing the data\n",
        "# you can adjust for your case\n",
        "authors = author_rdd.map(lambda r: (r[0],1)).reduceByKey(lambda a,b: a+b)\n",
        "author_30 = set(authors.filter(lambda r: r[1] >= 30).map(lambda r: r[0]).collect())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V4jrVTZm7NJn",
        "outputId": "14604497-ec76-4419-d9e7-7edf51e417e3"
      },
      "outputs": [],
      "source": [
        "title_author = author_rdd.filter(lambda r: r[0] in author_30). \\\n",
        "                    map(lambda r: (r[0],r[2])).distinct()\n",
        "title_author.take(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "97acIVei7NJo",
        "outputId": "aeecc32d-4371-4cde-9b13-0ec4079d9d01"
      },
      "outputs": [],
      "source": [
        "print(author_rdd.count())\n",
        "print(title_author.count())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qHwbHeY77NJo"
      },
      "source": [
        "## A1. Parse the data\n",
        "\n",
        "Here we make use of the natural language processing module `nltk`. Please download both the module and the corresponding data. See https://www.nltk.org/install.html and https://www.nltk.org/data.html for more details."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "6M_qGmj-7NJp"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4kE5SGl7NJp"
      },
      "source": [
        "## A2. Convert tokens into sparse vectors\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sSN7zYvT7NJp",
        "outputId": "01a49e17-f0e1-4d8c-8db1-9132b443e795"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xjP2RHegERud"
      },
      "source": [
        "## Generate a vectorized representation of the *tokens*\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRHdUsAx7NJq",
        "outputId": "2c9d60e4-b7b9-4500-9009-4eee5e2a76b7"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxS38Bsj7NJq"
      },
      "source": [
        "## Convert pyspark.ml vectors to pyspark.mllib vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhWffk1r7NJq"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X0JxSajd7NJr"
      },
      "source": [
        "## Check the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ROccPfR27NJr",
        "outputId": "d95bcfbd-c4cd-4f35-b90b-b4f19b68956d"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JHmr73aL7NJr"
      },
      "source": [
        "## A3. Latent Dirichlet Allocation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "z-D8pLq07NJr"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9roli0F7NJs"
      },
      "source": [
        "Looking at the topics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k9VM2LMi7NJs",
        "outputId": "1466efd5-c7bd-40db-e3dd-97f91392b4d5"
      },
      "outputs": [],
      "source": [
        "# Describe topics\n",
        "# your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "By2qFduy7NJs",
        "outputId": "a9ec1662-3ca5-4258-e429-7e79bdf494a2",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Shows the results\n",
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5jJOFTKGWSe"
      },
      "source": [
        "## A4. Comment your results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0fIECxa_7NJs"
      },
      "source": [
        "## B1. Convert tokens into sparse vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wQxKrPsD7NJt"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VkqEpsAHfBC"
      },
      "source": [
        "## Generate a vectorized representation of the *tokens*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bs-UsHT7NJu",
        "outputId": "e9d54251-c37d-4b6c-bfc0-81f48a29d45d"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdVwBNdt7NJu"
      },
      "source": [
        "## Convert pyspark.ml vectors to pyspark.mllib vectors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6MpyTa5q7NJv",
        "outputId": "c211e3a6-9049-49fc-9d7f-00d674f4b7f8"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CatbxUVB7NJv"
      },
      "source": [
        "### Take a look at the vocabulary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cgSlC5nT7NJv",
        "outputId": "a269f005-d5c1-4267-ef90-90583853c984"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWAqVT027NJw"
      },
      "source": [
        "## B1. Latent Dirichlet Allocation\n",
        "\n",
        "We now analyse the same dataset but using the Latent Dirichlet Allocation to find feature vectors characterizing topics of documents, and feature vectors characterizing the words of topics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "3F4twedy7NJw"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AitucvvP7NJw"
      },
      "source": [
        "The perplexity below is a measurement of how well a probability model predicts a sample. It may be used to compare probability models. A low perplexity indicates the probability distribution is good at predicting the sample."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4HCsUrS7NJx",
        "outputId": "b6e4db34-1c68-418b-bbf6-0ec64f63fcee"
      },
      "outputs": [],
      "source": [
        "# Describe topics and top-weighted terms\n",
        "# your code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8FQh8Slh7NJx",
        "outputId": "ea88596f-32ac-4d95-d46c-ea414ec0a5b4",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Shows the results\n",
        "# your code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8GIlHFm2IM1R"
      },
      "source": [
        "## B2. Calculate the topic density vector for each author and the cosine similarity ## "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KhEkCGfm7NJx"
      },
      "outputs": [],
      "source": [
        "# your code\n",
        "def dot_prod(x,y):\n",
        "    \n",
        "# your code\n",
        "def cos_sim(x,y):\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YcMjLVY1JXNG"
      },
      "source": [
        "## B3. Show the 10 most similar author pairs and comment on their similarity,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELy44cw07NJy",
        "outputId": "877a4bf8-61c6-4a26-c999-d7732a0e0190"
      },
      "outputs": [],
      "source": [
        "# your code"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw_dblp.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
